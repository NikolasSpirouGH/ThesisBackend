#!/usr/bin/env python3
"""
Standardized prediction script that works with any trained model.
This file is automatically generated and should not be modified by users.
"""

import json
import pickle
import pandas as pd
import numpy as np
import sys
import os

def main():
    try:
        # Use environment variables or fallback defaults (same convention as train.py)
        data_dir = os.getenv('DATA_DIR', '/data')
        model_dir = os.getenv('MODEL_DIR', '/model')

        print(f"Using DATA_DIR: {data_dir}")
        print(f"Using MODEL_DIR: {model_dir}")

        # Load the trained model
        model_path = os.path.join(model_dir, 'trained_model.pkl')
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Trained model not found at {model_path}")

        with open(model_path, 'rb') as f:
            model = pickle.load(f)

        print("Model loaded successfully!")

        # Load label mapping if it exists (for converting numeric predictions to class names)
        label_mapping = None
        label_mapping_path = os.path.join(model_dir, 'label_mapping.json')
        if os.path.exists(label_mapping_path):
            with open(label_mapping_path, 'r') as f:
                label_mapping = json.load(f)
            print(f"Label mapping loaded: {label_mapping}")

        # Load feature columns (for applying same encoding as training)
        feature_columns = None
        feature_columns_path = os.path.join(model_dir, 'feature_columns.json')
        if os.path.exists(feature_columns_path):
            with open(feature_columns_path, 'r') as f:
                feature_columns = json.load(f)
            print(f"Feature columns loaded: {len(feature_columns)} columns")

        # Load test dataset
        test_data_path = os.path.join(data_dir, 'test_data.csv')
        if not os.path.exists(test_data_path):
            raise FileNotFoundError(f"Test data not found at {test_data_path}")

        df = pd.read_csv(test_data_path)

        # Extract features (assume same structure as training data)
        # If last column contains '?' or similar placeholders, assume it's a target column and drop it
        if df.iloc[:, -1].astype(str).str.contains(r'^\?$', na=False).all():
            print(f"Detected placeholder target column (last column), dropping it for prediction")
            X_test_raw = df.iloc[:, :-1]
        else:
            X_test_raw = df

        # Apply same categorical encoding as training
        if feature_columns is not None:
            # Detect categorical columns
            categorical_cols = X_test_raw.select_dtypes(include=['object', 'category']).columns.tolist()

            if categorical_cols:
                print(f"Encoding categorical features: {categorical_cols}")
                # One-hot encode with same columns as training
                X_test_encoded = pd.get_dummies(X_test_raw, columns=categorical_cols, prefix_sep='_')

                # Ensure same columns as training (add missing, remove extra)
                for col in feature_columns:
                    if col not in X_test_encoded.columns:
                        X_test_encoded[col] = 0  # Add missing column with zeros

                # Reorder columns to match training
                X_test_encoded = X_test_encoded[feature_columns]
                X_test_df = X_test_encoded
                X_test = X_test_encoded.values
                print(f"Features encoded. Shape: {X_test.shape}")
            else:
                X_test_df = X_test_raw
                X_test = X_test_raw.values
        else:
            X_test_df = X_test_raw
            X_test = X_test_raw.values

        print(f"Test data loaded: {X_test.shape[0]} samples, {X_test.shape[1]} features")

        # Make predictions
        print("Making predictions...")
        predictions = model.predict(X_test)

        # Convert to standard format
        if hasattr(predictions, 'tolist'):
            predictions = predictions.tolist()
        elif not isinstance(predictions, list):
            predictions = list(predictions)

        # Convert numeric predictions back to class names if label mapping exists
        if label_mapping is not None:
            print(f"Converting numeric predictions to class names using label mapping...")
            predictions = [label_mapping[int(pred)] if isinstance(pred, (int, np.integer, float, np.floating)) else pred
                          for pred in predictions]
            print(f"Sample predictions after conversion: {predictions[:5]}")

        print(f"Generated {len(predictions)} predictions")

        # Ensure output directory exists
        os.makedirs(model_dir, exist_ok=True)

        # Save predictions to CSV with ORIGINAL input features in model directory (where service expects output)
        # Use the original raw features (before encoding), not the one-hot encoded ones
        output_df = X_test_raw.copy()
        output_df['prediction'] = predictions

        output_path = os.path.join(model_dir, 'predictions.csv')
        output_df.to_csv(output_path, index=False)

        # Save prediction metadata
        metadata = {
            'n_predictions': len(predictions),
            'status': 'success',
            'output_file': 'predictions.csv'
        }

        metadata_path = os.path.join(model_dir, 'prediction_metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        print(f"Predictions saved to {output_path}")
        print("Prediction completed successfully!")

    except Exception as e:
        print(f"Prediction failed: {str(e)}")

        model_dir = os.getenv('MODEL_DIR', '/model')

        # Save error metadata
        error_metadata = {
            'status': 'failed',
            'error': str(e)
        }

        os.makedirs(model_dir, exist_ok=True)
        error_path = os.path.join(model_dir, 'prediction_metadata.json')
        with open(error_path, 'w') as f:
            json.dump(error_metadata, f, indent=2)

        sys.exit(1)

if __name__ == '__main__':
    main()
