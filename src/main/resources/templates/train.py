#!/usr/bin/env python3
"""
Standardized training script that works with any user algorithm.
This file is automatically generated and should not be modified by users.
"""

import json
import pickle
import pandas as pd
import sys
import os
from pathlib import Path

def main():
    try:
        # Use environment variables or fallback to /data and /model
        data_dir = os.getenv('DATA_DIR', '/data')
        model_dir = os.getenv('MODEL_DIR', '/model')

        print(f"Using DATA_DIR: {data_dir}")
        print(f"Using MODEL_DIR: {model_dir}")

        # Load parameters
        params_path = os.path.join(data_dir, 'params.json')
        with open(params_path, 'r') as f:
            params = json.load(f)

        # Load dataset
        dataset_path = os.path.join(data_dir, 'dataset.csv')
        if not os.path.exists(dataset_path):
            raise FileNotFoundError(f"Dataset not found at {dataset_path}")

        df = pd.read_csv(dataset_path)

        # Split features and target
        # Assume last column is target, rest are features
        X_raw = df.iloc[:, :-1]
        y_raw = df.iloc[:, -1]

        # Convert nominal/categorical FEATURES to numeric using one-hot encoding
        feature_columns = []
        categorical_cols = X_raw.select_dtypes(include=['object', 'category']).columns.tolist()

        if categorical_cols:
            print(f"Detected categorical features: {categorical_cols}")
            # One-hot encode categorical features
            X_encoded = pd.get_dummies(X_raw, columns=categorical_cols, prefix_sep='_')
            feature_columns = X_encoded.columns.tolist()
            X = X_encoded.values
            print(f"Converted categorical features. New feature count: {len(feature_columns)}")
        else:
            feature_columns = X_raw.columns.tolist()
            X = X_raw.values
            print("All features are numeric, no conversion needed")

        # Convert target to numeric if it's categorical (strings)
        label_mapping = None
        if y_raw.dtype == 'object' or y_raw.dtype.name == 'category':
            # Map categorical values to numeric (0, 1, 2, ...)
            y, label_mapping_index = pd.factorize(y_raw)
            label_mapping = label_mapping_index.tolist()  # Convert to list for JSON serialization
            print(f"Converted categorical target: {dict(enumerate(label_mapping))}")
        else:
            y = y_raw.values

        print(f"Dataset loaded: {X.shape[0]} samples, {X.shape[1]} features")
        print(f"Parameters: {params}")

        # Import user's algorithm
        sys.path.insert(0, data_dir)
        from algorithm import Algorithm

        # Initialize and train the algorithm
        model = Algorithm(params)
        print("Training started...")
        model.fit(X, y)
        print("Training completed!")

        # Ensure model directory exists
        os.makedirs(model_dir, exist_ok=True)

        # Save the trained model
        model_path = os.path.join(model_dir, 'trained_model.pkl')
        with open(model_path, 'wb') as f:
            pickle.dump(model, f)

        print(f"Model saved to {model_path}")

        # Save label mapping if it exists (for converting predictions back to class names)
        if label_mapping is not None:
            label_mapping_path = os.path.join(model_dir, 'label_mapping.json')
            with open(label_mapping_path, 'w') as f:
                json.dump(label_mapping, f, indent=2)
            print(f"Label mapping saved to {label_mapping_path}: {label_mapping}")

        # Save feature columns (for applying same encoding during prediction)
        if feature_columns:
            feature_columns_path = os.path.join(model_dir, 'feature_columns.json')
            with open(feature_columns_path, 'w') as f:
                json.dump(feature_columns, f, indent=2)
            print(f"Feature columns saved to {feature_columns_path}: {len(feature_columns)} columns")

        # Save training metadata
        metadata = {
            'n_samples': int(X.shape[0]),
            'n_features': int(X.shape[1]),
            'params': params,
            'status': 'success'
        }

        metadata_path = os.path.join(model_dir, 'metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        # Save metrics file (required by the system)
        metrics = {
            'accuracy': 'N/A',  # Would need validation set to compute
            'loss': 'N/A',      # Final training loss if available
            'training_samples': int(X.shape[0]),
            'features': int(X.shape[1]),
            'epochs_completed': params.get('n_epochs', 'unknown'),
            'status': 'completed'
        }

        metrics_path = os.path.join(model_dir, 'metrics.json')
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f, indent=2)

        print("Training completed successfully!")

    except Exception as e:
        print(f"Training failed: {str(e)}")
        # Save error metadata
        model_dir = os.getenv('MODEL_DIR', '/model')
        error_metadata = {
            'status': 'failed',
            'error': str(e)
        }

        os.makedirs(model_dir, exist_ok=True)
        error_path = os.path.join(model_dir, 'metadata.json')
        with open(error_path, 'w') as f:
            json.dump(error_metadata, f, indent=2)

        sys.exit(1)

if __name__ == '__main__':
    main()